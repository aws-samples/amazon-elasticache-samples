# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0

import os
import logging
from dotenv import load_dotenv
from langchain.memory import ConversationSummaryBufferMemory, RedisChatMessageHistory
from langchain.llms.bedrock import Bedrock
from langchain.chains import ConversationChain
from langchain.prompts.prompt import PromptTemplate

# Load environment variables from .env file
load_dotenv()

# Set up logging
logging.basicConfig(level=logging.INFO)

# Get Redis URL from environment variable
redis_url = os.environ.get("ELASTICACHE_ENDPOINT_URL")


def get_llm():
    """
    Returns an instance of the Bedrock LLM with the specified model and configuration.
    Amazon Bedrock endpoints and quotas => https://docs.aws.amazon.com/general/latest/gr/bedrock.html

    Returns:
        Bedrock: An instance of the Bedrock LLM.
    """
    model_kwargs = {
        "max_tokens_to_sample": 8000,
        "temperature": 0.5,
        "top_k": 50,
        "top_p": 1,
        "stop_sequences": ["\n\nHuman:"]
    }
    llm = Bedrock(
        credentials_profile_name=os.getenv("BWB_PROFILE_NAME"),
        region_name=os.getenv("BWB_REGION_NAME"),
        endpoint_url=os.getenv("BWB_ENDPOINT_URL"),
        model_id="anthropic.claude-v2",
        model_kwargs=model_kwargs)
    return llm


def get_chat_history():
    """
    Returns an instance of RedisChatMessageHistory for storing chat history.

    Returns:
        RedisChatMessageHistory: An instance of RedisChatMessageHistory.
    """
    chat_history = RedisChatMessageHistory(session_id='username', url=redis_url, key_prefix="chat_history:")
    return chat_history


def get_memory(session_id, url, key_prefix):
    """
    Creates and returns a ConversationSummaryBufferMemory instance for maintaining conversation history.

    Args:
        session_id (str): The session ID for the chat session.
        url (str): The URL of the Redis instance.
        key_prefix (str): The key prefix for storing chat history in Redis.

    Returns:
        ConversationSummaryBufferMemory: An instance of ConversationSummaryBufferMemory.
    """
    # ConversationSummaryBufferMemory requires an LLM for summarizing older messages
    # This allows us to maintain the "big picture" of a long-running conversation
    llm = get_llm()
    chat_history = RedisChatMessageHistory(session_id=session_id, url=url, key_prefix=key_prefix)
    memory = ConversationSummaryBufferMemory(ai_prefix="AI Assistant", llm=llm, max_token_limit=1024, chat_memory=chat_history)
    return memory


def get_chat_response(input_text, memory):
    """
    Generates a chat response based on the input text and conversation history.

    Args:
        input_text (str): The input text from the user.
        memory (ConversationSummaryBufferMemory): The conversation history and summary.

    Returns:
        str: The chat response generated by the LLM.
    """
    llm = get_llm()
    template = """The following is a friendly conversation between a human and an AI.
    AI provide very concise responses. If the AI does not know the answer to a question, it truthfully says it does not know.
    Current conversation:{history}. Human: {input}  AI Assistant:"""
    PROMPT = PromptTemplate(input_variables=["history", "input"], template=template)
    conversation_with_summary = ConversationChain(
        llm=llm,        # using the Bedrock LLM
        memory=memory,  # with the summarization memory
        prompt=PROMPT,
        verbose=False   # disable printing internal states
    )
    chat_response = conversation_with_summary.predict(input=input_text)  # pass the user message and summary to the model
    logging.debug(f"Chat response: {chat_response}")
    return chat_response
