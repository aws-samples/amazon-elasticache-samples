{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a74501",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRecursionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdecimal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Decimal\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tools/github/.venv/lib/python3.13/site-packages/numpy/__init__.py:472\u001b[39m\n\u001b[32m    469\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.platform == \u001b[33m\"\u001b[39m\u001b[33mdarwin\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exceptions\n\u001b[32m    473\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings(record=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m w:\n\u001b[32m    474\u001b[39m         _mac_os_check()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tools/github/.venv/lib/python3.13/site-packages/numpy/__init__.py:352\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ctypeslib\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33mexceptions\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexceptions\u001b[39;00m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exceptions\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33mtesting\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tools/github/.venv/lib/python3.13/site-packages/numpy/__init__.py:352\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ctypeslib\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33mexceptions\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexceptions\u001b[39;00m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exceptions\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33mtesting\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "    \u001b[31m[... skipping similar frames: __getattr__ at line 352 (2963 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tools/github/.venv/lib/python3.13/site-packages/numpy/__init__.py:352\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ctypeslib\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33mexceptions\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexceptions\u001b[39;00m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exceptions\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33mtesting\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:471\u001b[39m, in \u001b[36m_lock_unlock_module\u001b[39m\u001b[34m(name)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:311\u001b[39m, in \u001b[36macquire\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:170\u001b[39m, in \u001b[36m__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:132\u001b[39m, in \u001b[36msetdefault\u001b[39m\u001b[34m(self, key, default)\u001b[39m\n",
      "\u001b[31mRecursionError\u001b[39m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import boto3\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import string\n",
    "import random\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse command line arguments\n",
    "parser = argparse.ArgumentParser(description='Collect AWS ElastiCache metrics with specific aggregation rules.')\n",
    "parser.add_argument('-r', '--region', required=True, help='AWS region for the ElastiCache cluster')\n",
    "parser.add_argument('-c', '--cluster', required=True, help='ElastiCache cluster ID')\n",
    "parser.add_argument('-dr', '--day-range', type=int, default=1, help='Day range for hourly metrics collection <1>')\n",
    "parser.add_argument('-o', '--output', required=False, help='Output CSV file name <output.csv>')\n",
    "args = parser.parse_args()\n",
    "\n",
    "if not args.region:\n",
    "    print(\"ERROR: Missing region parameter. Please pass in the region name [US-EAST-1, US-EAST-2, and so on]\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Region Name: {args.region}\")\n",
    "if not args.cluster:\n",
    "    print(\"ERROR: Missing cluster name parameter. Please pass in the cluster name <example-elasti-cache>\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Cluster Name: {args.cluster}\")\n",
    "\n",
    "if not args.output:\n",
    "    args.output = 'cost_estimate_' + args.cluster + '_' + datetime.now().strftime(\"%H:%M_%d_%m_%Y\") + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELASTICACHE_SERVERLESS_PRICING = {\n",
    "    # US Regions\n",
    "    \"us-east-1\": {  # US East (N. Virginia)\n",
    "        \"ecpu_per_million\": 2.30,\n",
    "        \"storage_per_gb\": 0.084\n",
    "    },\n",
    "    \"us-east-2\": {  # US East (Ohio)\n",
    "        \"ecpu_per_million\": 2.30,\n",
    "        \"storage_per_gb\": 0.084\n",
    "    },\n",
    "    \"us-west-1\": {  # US West (N. California)\n",
    "        \"ecpu_per_million\": 2.76,\n",
    "        \"storage_per_gb\": 0.101\n",
    "    },\n",
    "    \"us-west-2\": {  # US West (Oregon)\n",
    "        \"ecpu_per_million\": 2.30,\n",
    "        \"storage_per_gb\": 0.084\n",
    "    },\n",
    "    \n",
    "    # Canada Region\n",
    "    \"ca-central-1\": {  # Canada (Central)\n",
    "        \"ecpu_per_million\": 2.53,\n",
    "        \"storage_per_gb\": 0.092\n",
    "    },\n",
    "    \n",
    "    # South America Region\n",
    "    \"sa-east-1\": {  # South America (SÃ£o Paulo)\n",
    "        \"ecpu_per_million\": 3.45,\n",
    "        \"storage_per_gb\": 0.126\n",
    "    },\n",
    "    \n",
    "    # Europe Regions\n",
    "    \"eu-central-1\": {  # Europe (Frankfurt)\n",
    "        \"ecpu_per_million\": 2.53,\n",
    "        \"storage_per_gb\": 0.092\n",
    "    },\n",
    "    \"eu-west-1\": {  # Europe (Ireland)\n",
    "        \"ecpu_per_million\": 2.30,\n",
    "        \"storage_per_gb\": 0.084\n",
    "    },\n",
    "    \"eu-west-2\": {  # Europe (London)\n",
    "        \"ecpu_per_million\": 2.42,\n",
    "        \"storage_per_gb\": 0.088\n",
    "    },\n",
    "    \"eu-west-3\": {  # Europe (Paris)\n",
    "        \"ecpu_per_million\": 2.42,\n",
    "        \"storage_per_gb\": 0.088\n",
    "    },\n",
    "    \"eu-north-1\": {  # Europe (Stockholm)\n",
    "        \"ecpu_per_million\": 2.19,\n",
    "        \"storage_per_gb\": 0.080\n",
    "    },\n",
    "    \"eu-south-1\": {  # Europe (Milan)\n",
    "        \"ecpu_per_million\": 2.53,\n",
    "        \"storage_per_gb\": 0.092\n",
    "    },\n",
    "    \n",
    "    # Asia Pacific Regions\n",
    "    \"ap-east-1\": {  # Asia Pacific (Hong Kong)\n",
    "        \"ecpu_per_million\": 3.00,\n",
    "        \"storage_per_gb\": 0.109\n",
    "    },\n",
    "    \"ap-south-1\": {  # Asia Pacific (Mumbai)\n",
    "        \"ecpu_per_million\": 2.53,\n",
    "        \"storage_per_gb\": 0.092\n",
    "    },\n",
    "    \"ap-northeast-1\": {  # Asia Pacific (Tokyo)\n",
    "        \"ecpu_per_million\": 2.76,\n",
    "        \"storage_per_gb\": 0.101\n",
    "    },\n",
    "    \"ap-northeast-2\": {  # Asia Pacific (Seoul)\n",
    "        \"ecpu_per_million\": 2.53,\n",
    "        \"storage_per_gb\": 0.092\n",
    "    },\n",
    "    \"ap-northeast-3\": {  # Asia Pacific (Osaka)\n",
    "        \"ecpu_per_million\": 2.76,\n",
    "        \"storage_per_gb\": 0.101\n",
    "    },\n",
    "    \"ap-southeast-1\": {  # Asia Pacific (Singapore)\n",
    "        \"ecpu_per_million\": 2.76,\n",
    "        \"storage_per_gb\": 0.101\n",
    "    },\n",
    "    \"ap-southeast-2\": {  # Asia Pacific (Sydney)\n",
    "        \"ecpu_per_million\": 2.76,\n",
    "        \"storage_per_gb\": 0.101\n",
    "    },\n",
    "    \n",
    "    # Middle East Regions\n",
    "    \"me-south-1\": {  # Middle East (Bahrain)\n",
    "        \"ecpu_per_million\": 2.76,\n",
    "        \"storage_per_gb\": 0.101\n",
    "    },\n",
    "    \n",
    "    # Africa Regions\n",
    "    \"af-south-1\": {  # Africa (Cape Town)\n",
    "        \"ecpu_per_million\": 2.76,\n",
    "        \"storage_per_gb\": 0.101\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize AWS clients\n",
    "try:\n",
    "    elasticache = boto3.client('elasticache', region_name=args.region)\n",
    "    cloudwatch = boto3.client('cloudwatch', region_name=args.region)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing AWS client, credential are probably missing: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "def calculate_total_costs(df):\n",
    "    \"\"\"Calculate total costs across the date range\"\"\"\n",
    "    total_storage_cost = Decimal(str(df['StorageCost'].sum()))\n",
    "    total_cpu_cost = Decimal(str(df['eCPUCost'].sum()))\n",
    "    total_cost = Decimal(str(df['TotalCost'].sum()))\n",
    "    \n",
    "    return {\n",
    "        'storage_cost': total_storage_cost,\n",
    "        'cpu_cost': total_cpu_cost,\n",
    "        'total_cost': total_cost,\n",
    "        'hours_analyzed': len(df)\n",
    "    }\n",
    "\n",
    "def get_nodes(cluster_id):\n",
    "    \"\"\"Retrieve primary nodes considering cluster mode enabled scenarios.\"\"\"\n",
    "    try:\n",
    "        response = elasticache.describe_replication_groups(ReplicationGroupId=cluster_id)\n",
    "        all_nodes = response['ReplicationGroups'][0].get('MemberClusters', [])\n",
    "        return all_nodes\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving cluster node details: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def get_metric_data(metric_name, node_id, start_time, end_time, period=3600, stat='Average'):\n",
    "    \"\"\"Aggregate metric data for given node ID over the specified time range.\"\"\"\n",
    "    aggregated_data = {}\n",
    "    \n",
    "    response = cloudwatch.get_metric_statistics(\n",
    "        Namespace='AWS/ElastiCache',\n",
    "        MetricName=metric_name,\n",
    "        Dimensions=[{'Name': 'CacheClusterId', 'Value': node_id}],\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time,\n",
    "        Period=period,  # default 3600 seconds or 1 hour\n",
    "        Statistics=[stat],\n",
    "    )\n",
    "\n",
    "    for datapoint in response['Datapoints']:\n",
    "        timestamp = datapoint['Timestamp'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        if timestamp not in aggregated_data:\n",
    "            aggregated_data[timestamp] = datapoint[stat]\n",
    "        else:\n",
    "            aggregated_data[timestamp] += datapoint[stat]\n",
    "    return aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_write_metrics(cluster_id, start_time, end_time, filename):\n",
    "    \"\"\"Main function that collects, displays, and saves metrics data to a csv file.\"\"\"\n",
    "    primary_nodes = []\n",
    "    reader_nodes = []\n",
    "    \n",
    "    all_nodes = get_nodes(cluster_id)\n",
    "\n",
    "    # Identify the primary and read replica nodes\n",
    "    try:\n",
    "        # Generate a list of current primary and read replica nodes\n",
    "        # based on the role each cluster node played in the last minute\n",
    "        l_start_time = end_time - timedelta(minutes=1)\n",
    "        for node in all_nodes:\n",
    "            aggregated_data = get_metric_data('IsMaster', node, l_start_time, end_time, 60, 'Sum')\n",
    "            if next(iter(aggregated_data.values())) == 1.0:\n",
    "                primary_nodes.append(node)\n",
    "            else:\n",
    "                reader_nodes.append(node)\n",
    "    except Exception as e:\n",
    "        print(f\"No metrics exist for cluster: {cluster_id}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    primary_node = primary_nodes[0]\n",
    "    print(\"Primary node used: \" + primary_node)\n",
    "\n",
    "    if len(reader_nodes) > 1:\n",
    "        reader_node = reader_nodes[0]\n",
    "        print(\"Reader node used: \" + reader_node)\n",
    "    else:\n",
    "        reader_node = None\n",
    "\n",
    "    num_shards = len(primary_nodes)\n",
    "    num_readers = len(reader_nodes)\n",
    "    num_replicas = int(num_readers/num_shards)\n",
    "\n",
    "    print(\"Number of primaries: \" + str(num_shards))\n",
    "    print(\"Number of replicas: \" + str(num_replicas))\n",
    "\n",
    "    # Prepare data structure for CSV writing\n",
    "    collected_data = {}\n",
    "\n",
    "    # For a primary node collect the following metrics\n",
    "    for metric in ['BytesUsedForCache', 'EvalBasedCmds', 'EvalBasedCmdsLatency', 'GetTypeCmds', \n",
    "                   'NetworkBytesIn', 'NetworkBytesOut', 'ReplicationBytes', 'SetTypeCmds']:\n",
    "        # Retrieve the average for the below metrics\n",
    "        if metric in ['BytesUsedForCache', 'EvalBasedCmdsLatency']:\n",
    "            aggregated_data = get_metric_data(metric, primary_node, start_time, end_time, stat='Average')\n",
    "        # The sum for the rest of the metrics\n",
    "        else:\n",
    "            aggregated_data = get_metric_data(metric, primary_node, start_time, end_time, stat='Sum')\n",
    "\n",
    "        for timestamp, value in aggregated_data.items():\n",
    "            if timestamp not in collected_data:\n",
    "                collected_data[timestamp] = {}\n",
    "            collected_data[timestamp][metric] = value\n",
    "\n",
    "    # For a read replica node only the GetTypeCmds and NetworkBytesOut metrics are needed\n",
    "    if reader_node is not None:\n",
    "        for metric in ['GetTypeCmds', 'NetworkBytesOut']:\n",
    "            aggregated_data = get_metric_data(metric, reader_node, start_time, end_time, stat='Sum')\n",
    "            reader_metric = 'Reader' + metric\n",
    "            for timestamp, value in aggregated_data.items():\n",
    "                collected_data[timestamp][reader_metric] = value\n",
    "\n",
    "    dataKeys = list(collected_data.keys())\n",
    "    dataKeys.sort()\n",
    "    sorted_collected_data = {i: collected_data[i] for i in dataKeys}\n",
    "\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    df = pd.DataFrame(sorted_collected_data)\n",
    "    df = df.transpose()\n",
    "\n",
    "    # Since certain fields might not be populated, for lack of data, set them to 0\n",
    "    df['GetTypeCmds'] = df.get('GetTypeCmds', 0)\n",
    "    df['SetTypeCmds'] = df.get('SetTypeCmds', 0)\n",
    "    df['EvalBasedCmds'] = df.get('EvalBasedCmds', 0)\n",
    "    df['EvalBasedCmdsLatency'] = df.get('EvalBasedCmdsLatency', 0)\n",
    "    df['ReaderGetTypeCmds'] = df.get('ReaderGetTypeCmds', 0)\n",
    "    df['ReaderNetworkBytesOut'] = df.get('ReaderNetworkBytesOut', 0)\n",
    "    df['ReplicationBytes'] = df.get('ReplicationBytes', 0)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    columns = ['BytesUsedForCache', 'EvalBasedCmds', 'EvalBasedCmdsLatency', 'GetTypeCmds', \n",
    "               'ReaderGetTypeCmds', 'NetworkBytesIn', 'NetworkBytesOut', 'ReaderNetworkBytesOut', \n",
    "               'ReplicationBytes', 'SetTypeCmds']\n",
    "    df = df[columns]\n",
    "    \n",
    "    df['TotalSizeMB'] = df['BytesUsedForCache'].div(1000*1000).mul(num_shards).round(2).apply(lambda x : \"{:,}\".format(x))\n",
    "    df['EvaleCPU'] = (df['EvalBasedCmds'].mul(num_shards) * df['EvalBasedCmdsLatency'].div(2)).astype(int)\n",
    "    df['EvalBasedCmds'] = df['EvalBasedCmds'].mul(num_shards)\n",
    "    \n",
    "    # Prevent division by 0\n",
    "    df['AVGInSize'] = np.where(df['SetTypeCmds'] == 0, 0,\n",
    "                              df['NetworkBytesIn'].div(1000)/df['SetTypeCmds'].astype(int))\n",
    "    \n",
    "    df['PrimaryIneCPU'] = np.where((df['AVGInSize'] > 0) & (df['AVGInSize'] < 1), \n",
    "                                  df['SetTypeCmds'] * num_shards,\n",
    "                                  df['SetTypeCmds'] * df['AVGInSize'] * num_shards)\n",
    "\n",
    "    df['GetTypeCmds'] = df['GetTypeCmds'].astype(int)\n",
    "    df['ReaderGetTypeCmds'] = df['ReaderGetTypeCmds'].astype(int)\n",
    "\n",
    "    df['AVGOutSize'] = np.where(df['GetTypeCmds'] == 0, 0,\n",
    "                               ((df['NetworkBytesOut'].div(1000))-\n",
    "                                (df['ReplicationBytes'].div(1000).mul(num_readers)))/\n",
    "                               df['GetTypeCmds'].astype(int))\n",
    "\n",
    "    df['ReaderAVGOutSize'] = np.where(df['ReaderGetTypeCmds'] == 0, 0,\n",
    "                                     df['ReaderNetworkBytesOut'].div(1000)/\n",
    "                                     df['ReaderGetTypeCmds'].astype(int))\n",
    "\n",
    "    df['PrimaryOuteCPU'] = np.where((df['AVGOutSize'] > 0) & (df['AVGOutSize'] < 1), \n",
    "                                   df['GetTypeCmds'] * num_shards,\n",
    "                                   df['GetTypeCmds'] * df['AVGOutSize'] * num_shards)\n",
    "\n",
    "    df['ReaderOuteCPU'] = np.where((df['ReaderAVGOutSize'] > 0) & (df['ReaderAVGOutSize'] <= 1), \n",
    "                                  df['ReaderGetTypeCmds'].astype(int) * num_readers,\n",
    "                                  df['ReaderGetTypeCmds'].astype(int) * df['ReaderAVGOutSize'] * num_readers)\n",
    "\n",
    "    df['SetTypeCmds'] = df['SetTypeCmds'].astype(int)\n",
    "\n",
    "    # Minimum storage cost is for 100MB\n",
    "    df['StorageCost'] = np.where(df['BytesUsedForCache'].div(1000*1000).mul(num_shards).round(4) <= 100, \n",
    "                                (0.084),\n",
    "                                df['BytesUsedForCache'].div(1000*1000*1000).mul(num_shards).mul(0.084).round(2))\n",
    "\n",
    "    df['eCPUCost'] = (df['EvaleCPU'].mul(0.0000000023).apply(lambda x: round(x, 4)) + \n",
    "                      df['PrimaryIneCPU'].mul(0.0000000023).apply(lambda x: round(x, 4)) + \n",
    "                      df['PrimaryOuteCPU'].mul(0.0000000023).apply(lambda x: round(x, 4)) + \n",
    "                      df['ReaderOuteCPU'].mul(0.0000000023).apply(lambda x: round(x, 4)))\n",
    "    \n",
    "    df['TotalCost'] = (df['StorageCost'] + df['eCPUCost']).round(3)\n",
    "\n",
    "    print(\"\")\n",
    "    print(df[['TotalSizeMB', 'EvaleCPU', 'PrimaryIneCPU', 'PrimaryOuteCPU', 'ReaderOuteCPU', \n",
    "              'StorageCost', 'eCPUCost', 'TotalCost']])\n",
    "\n",
    "    # Calculate total costs across the date range\n",
    "    total_costs = calculate_total_costs(df)\n",
    "    \n",
    "    print(\"\\nSummary for the entire period:\")\n",
    "    print(f\"Total Hours Analyzed: {total_costs['hours_analyzed']}\")\n",
    "    print(f\"Total Cost: ${total_costs['total_cost']:.3f}\")\n",
    "    \n",
    "    # Add summary rows to DataFrame\n",
    "    summary_rows = [\n",
    "        pd.Series({\n",
    "            'TotalSizeMB': '',\n",
    "            'StorageCost': '',\n",
    "            'eCPUCost': '',\n",
    "            'TotalCost': ''\n",
    "        }, name=''),\n",
    "        pd.Series({\n",
    "            'TotalSizeMB': 'SUMMARY',\n",
    "            'StorageCost': '',\n",
    "            'eCPUCost': '',\n",
    "            'TotalCost': ''\n",
    "        }, name='Summary Statistics'),\n",
    "        pd.Series({\n",
    "            'TotalSizeMB': f'Total Hours Analyzed: {total_costs[\"hours_analyzed\"]}',\n",
    "            'StorageCost': '',\n",
    "            'eCPUCost': '',\n",
    "            'TotalCost': ''\n",
    "        }, name=''),\n",
    "        pd.Series({\n",
    "            'TotalSizeMB': f'Total Cost: ${total_costs[\"total_cost\"]:.3f}',\n",
    "            'StorageCost': '',\n",
    "            'eCPUCost': '',\n",
    "            'TotalCost': ''\n",
    "        }, name='')\n",
    "    ]\n",
    "\n",
    "    # Concatenate the original DataFrame with the summary rows\n",
    "    df = pd.concat([df] + [pd.DataFrame([row]) for row in summary_rows])\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        df.to_csv(csvfile, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7880c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    end_time = datetime.utcnow()\n",
    "    end_time = end_time.replace(minute=0, second=0)\n",
    "    start_time = end_time - timedelta(days=args.day_range)\n",
    "    print('Start time: ' + str(start_time))\n",
    "    print('End time: ' + str(end_time))\n",
    "    collect_and_write_metrics(args.cluster, start_time, end_time, args.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Github",
   "language": "python",
   "name": "guthub-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
